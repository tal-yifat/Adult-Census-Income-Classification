{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.options.display.float_format = '{:.3f}'.format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the data a divide into train and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((36631, 15), (12211, 15))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_1 = pd.read_csv(\"adult.data\", names=['age', 'workclass', 'fnlwgt', 'education', 'education-num', \n",
    "                                             'marital-status', 'occupation', 'relationship', 'race', 'sex', \n",
    "                                             'capital-gain', 'capital-loss', 'hours-per-week', 'native-country', \n",
    "                                             '<=50K'], skipinitialspace=True)\n",
    "df_2 = pd.read_csv(\"adult.test\", names=['age', 'workclass', 'fnlwgt', 'education', 'education-num', \n",
    "                                             'marital-status', 'occupation', 'relationship', 'race', 'sex', \n",
    "                                             'capital-gain', 'capital-loss', 'hours-per-week', 'native-country', \n",
    "                                             '<=50K'], skipinitialspace=True, skiprows=1)\n",
    "df_combined = df_1.append(df_2)\n",
    "\n",
    "df_combined = df_combined.sample(frac=1, random_state=0)\n",
    "\n",
    "df_train = df_combined.iloc[:int(df_combined.shape[0] * .75)].copy()\n",
    "df_test = df_combined.iloc[int(df_combined.shape[0] * .75):].copy()\n",
    "df_train.shape, df_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-process data using an automated data transformation pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped 648 rows with unknown values.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tal/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/data.py:625: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/Users/tal/anaconda3/lib/python3.7/site-packages/sklearn/base.py:462: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped 209 rows with unknown values.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tal/anaconda3/lib/python3.7/site-packages/sklearn/pipeline.py:451: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  Xt = transform.transform(Xt)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((35983, 44), (12002, 44), (35983,), (12002,))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from custom_transformers import NominalCategoryMerger, OrdinalCategoryMerger\n",
    "from custom_transformers import UnknownValuesDropper, CharacterStripper \n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "num_features = ['age', 'fnlwgt', 'education-num', 'capital-gain', 'capital-loss', 'hours-per-week']\n",
    "cat_features = ['workclass', 'marital-status', 'occupation', 'relationship', 'race', 'sex', \n",
    "                  'native-country']\n",
    "occ_features = ['education-num']\n",
    "label = '<=50K'\n",
    "\n",
    "df_train = df_train[num_features + cat_features + [label]]\n",
    "df_test = df_test[num_features + cat_features + [label]]\n",
    "\n",
    "ncc_threshold = None\n",
    "ncc_alpha = .1\n",
    "\n",
    "# Create and parametatrize data transformers\n",
    "dropper = UnknownValuesDropper(features=['native-country'], unknown_value='?')\n",
    "stripper = CharacterStripper([label], '.')\n",
    "nominal_merger = NominalCategoryMerger(cat_features, label, alpha=ncc_alpha)\n",
    "ordinal_merger = OrdinalCategoryMerger(occ_features, label, alpha=ncc_alpha)\n",
    "scaler = StandardScaler()\n",
    "encoder = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "num_pipeline = Pipeline([('StandardScaler', scaler)])\n",
    "cat_pipeline = Pipeline([('OneHotEncoder', encoder)])\n",
    "column_transformer = ColumnTransformer([('Numeric Pipeline', num_pipeline, num_features), \n",
    "                                        ('Categorical Pipeline', cat_pipeline, cat_features),\n",
    "                                        ('Label', encoder, [label]) ])\n",
    "\n",
    "# Define the data transformation pipeline\n",
    "X_pipeline = Pipeline([('UnknownValuesDropper', dropper), \n",
    "                       ('CharacterStripper', stripper), \n",
    "                       ('NominalCategoryMerger', nominal_merger), \n",
    "                       ('OrdinalCategoryMerger', ordinal_merger), \n",
    "                       ('ColumnTransformer', column_transformer)])\n",
    "\n",
    "# Fit and transform the data\n",
    "dataset_train = X_pipeline.fit_transform(df_train)\n",
    "dataset_test = X_pipeline.transform(df_test)\n",
    "\n",
    "X_train = dataset_train[:, :-2]\n",
    "y_train = dataset_train[:, -2].flatten()\n",
    "X_test = dataset_test[:, :-2]\n",
    "y_test = dataset_test[:, -2].flatten()\n",
    "\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary testing of models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8518189003195694"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "log_reg = LogisticRegressionCV(cv=5, random_state=0, max_iter=500).fit(X_train, y_train)\n",
    "score = cross_val_score(log_reg, X_train, y_train, cv=5, n_jobs=-1)\n",
    "np.average(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8551816789465538"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "random_forest = RandomForestClassifier(n_estimators=500, random_state=0, n_jobs=-1).fit(X_train, y_train)\n",
    "score = cross_val_score(random_forest, X_train, y_train, cv=5, n_jobs=-1)\n",
    "np.average(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8517910491808781"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "linear_svc = LinearSVC(random_state=0, tol=1e-5, max_iter=10000, C=.01).fit(X_train, y_train)\n",
    "score = cross_val_score(linear_svc, X_train, y_train, cv=5, n_jobs=-1)\n",
    "np.average(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8526805325680072"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "poly_svc = SVC(kernel='poly', gamma='auto', C=1000, random_state=0).fit(X_train, y_train)\n",
    "score = cross_val_score(poly_svc, X_train, y_train, cv=5, n_jobs=-1)\n",
    "np.average(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8573770806357407"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rbf_svc = SVC(kernel='rbf', gamma='auto', C=30, random_state=0).fit(X_train, y_train)\n",
    "score = cross_val_score(rbf_svc, X_train, y_train, cv=5, n_jobs=-1)\n",
    "np.average(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.845788146470913"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=100, weights='distance').fit(X_train, y_train)\n",
    "score = cross_val_score(knn, X_train, y_train, cv=5, n_jobs=-1)\n",
    "np.average(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8655198758163811"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "adaboost = AdaBoostClassifier(n_estimators=100).fit(X_train, y_train)\n",
    "score = cross_val_score(adaboost, X_train, y_train, cv=5, n_jobs=-1)\n",
    "np.average(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8667149031179537"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "gbc = GradientBoostingClassifier(learning_rate=0.1, n_estimators=100).fit(X_train, y_train)\n",
    "score = cross_val_score(gbc, X_train, y_train, cv=5, n_jobs=-1)\n",
    "np.average(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning the selected gradient boosted tree classifier\n",
    "The experiments shown above (plus some more that are not presented here) lead to the conclusion that Gradient Boosting Classifier has the best performance for this dataset. In this section, I tune the model following these steps:\n",
    "\n",
    "1. GBC performs better the lower its learning rate is, but requires more estimators (and therefore more training time) for lower learning rates. I will start by setting the learning rate of 0.1 and lower it on the final step. \n",
    "2. Find the optimal number of trees for this learning rate.\n",
    "3. Tune tree specific parameters.\n",
    "4. Lower the learning rate and increase the number of trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gbc_early_stopping_tuning(estimator, initial_n_estimators):\n",
    "    max_val_score = 0\n",
    "    best_n_estimators = initial_n_estimators\n",
    "    score_going_down = 0\n",
    "    estimator.n_estimators = initial_n_estimators\n",
    "    while score_going_down < 3:\n",
    "        estimator.fit(X_train, y_train)\n",
    "        val_score = np.average(cross_val_score(estimator, X_train, y_train, cv=5, n_jobs=-1))\n",
    "        print(estimator.n_estimators, val_score)\n",
    "        if val_score > max_val_score:\n",
    "            max_val_score = val_score\n",
    "            best_n_estimators =  estimator.n_estimators\n",
    "            score_going_down = 0\n",
    "        else:\n",
    "            score_going_down += 1\n",
    "        estimator.n_estimators = int(estimator.n_estimators * 1.2)\n",
    "\n",
    "    return best_n_estimators, max_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 0.8515130783828402\n",
      "24 0.8547645798937442\n",
      "28 0.8561263124443677\n",
      "33 0.8578215906510419\n",
      "39 0.8589332846332252\n",
      "46 0.860239504116435\n",
      "55 0.861767972811409\n",
      "66 0.86407467092986\n",
      "79 0.8653530971935425\n",
      "94 0.865936766368572\n",
      "112 0.8671039772831662\n",
      "134 0.868354548550671\n",
      "160 0.8696606599212565\n",
      "192 0.8705221299846629\n",
      "230 0.8718004713010256\n",
      "276 0.8726063626532252\n",
      "331 0.8724674815104171\n",
      "397 0.8726898465658731\n",
      "476 0.8726898427019488\n",
      "571 0.8727176783913808\n",
      "685 0.8721896111046659\n",
      "822 0.871550432728293\n",
      "986 0.8714670492079316\n",
      "1183 0.8701052510135149\n",
      "1419 0.8694382794133222\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(571, 0.8727176783913808)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimator = GradientBoostingClassifier(learning_rate=0.1)\n",
    "initial_n_estimators = 20\n",
    "gbc_early_stopping_tuning(estimator, initial_n_estimators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'max_depth': 3, 'max_features': None, 'subsample': 1}, 0.8726065086290749)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_test = {'max_depth':range(2, 8), 'max_features':[4, 5, 7, 10, 15, None], \n",
    "              'subsample':[0.6, 0.7, 0.8, 0.9, 1]}\n",
    "\n",
    "gsearch = GridSearchCV(estimator = GradientBoostingClassifier(learning_rate=0.1, n_estimators=500, random_state=0),\n",
    "                       param_grid = param_test, n_jobs=-1, cv=5)\n",
    "gsearch.fit(X_train, y_train)\n",
    "gsearch.best_params_, gsearch.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60 0.8625739800073007\n",
      "72 0.864658305347812\n",
      "86 0.8656032284371511\n",
      "103 0.8667426924746302\n",
      "123 0.8674651964511281\n",
      "147 0.8691326775807882\n",
      "176 0.8700496723055272\n",
      "211 0.8713002667383367\n",
      "253 0.8721617792770123\n",
      "303 0.8723840671033404\n",
      "363 0.8726064360195018\n",
      "435 0.8727176088911746\n",
      "522 0.872356370414321\n",
      "626 0.872745425273861\n",
      "751 0.8720506681819892\n",
      "901 0.8715226626740702\n",
      "1081 0.8705221763163454\n",
      "1297 0.8697439507557198\n",
      "1556 0.8691603665365937\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(626, 0.872745425273861)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimator = GradientBoostingClassifier(learning_rate=0.1, max_depth=3)\n",
    "initial_n_estimators = 60\n",
    "gbc_early_stopping_tuning(estimator, initial_n_estimators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = GradientBoostingClassifier(learning_rate=0.01, max_depth=3)\n",
    "initial_n_estimators = 1000\n",
    "gbc_early_stopping_tuning(estimator, initial_n_estimators)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now, I am skipping, for lack of time, the final tuning step - lowering the learning rate and adjusting the number of estimators accordingly. Also missing here is an evaluation of the selected model against the test set. The selected model performed on the test set drastically worse than on the validation set. I believe that the reason for this is a bug in the custom transfomers - I have received in some experiments test set results of over 87.5% accuracy. I will update the repo later addressing these issues.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
